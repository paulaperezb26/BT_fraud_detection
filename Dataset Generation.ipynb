{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports for this notebook\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "import faker \n",
    "from faker import Faker\n",
    "from collections import OrderedDict\n",
    "# For plotting\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "locales=OrderedDict([('es_ES', 3)])\n",
    "faker= Faker(locales)\n",
    "Faker.seed(0)\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sender Profile Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_customer_profiles_table(n_customers, random_state=0):\n",
    "    np.random.seed(random_state)\n",
    "        \n",
    "    customer_id_properties=[]\n",
    "    # Generate customer properties from random distributions \n",
    "    for customer_id in range(n_customers):\n",
    "        #acc_id=np.random.randint(1,n_customers)\n",
    "        acc_id=faker['es_ES'].iban()\n",
    "        name=faker['es_ES'].first_name()\n",
    "        last_name=faker['es_ES'].last_name()\n",
    "        dob=faker.date_of_birth(minimum_age=18, maximum_age=27)\n",
    "        mean_amount = np.random.uniform(0.50,1000) # Arbitrary (but sensible) value \n",
    "        std_amount = mean_amount/2 # Arbitrary (but sensible) value\n",
    "        mean_nb_tx_per_day = np.random.uniform(0,4) # Arbitrary (but sensible) value \n",
    "        ip=faker.ipv4()\n",
    "        customer_id_properties.append([customer_id, acc_id, name, last_name,dob, ip,\n",
    "                                      mean_amount, std_amount,\n",
    "                                      mean_nb_tx_per_day])\n",
    "        \n",
    "    customer_profiles_table = pd.DataFrame(customer_id_properties, columns=['SENDER_ID','iban','name','last_name', 'dob','ip',                                                                            \n",
    "                                                                      'mean_amount', 'std_amount',\n",
    "                                                                      'mean_nb_tx_per_day'])\n",
    "    \n",
    "    return customer_profiles_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing sender generating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_customers = 4\n",
    "customer_profiles_table = generate_customer_profiles_table(n_customers, random_state = 0)\n",
    "customer_profiles_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Profile Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_receiver_profiles_table(n_receiver, random_state=0):\n",
    "    locales=OrderedDict([('es_ES', 2), ('it_IT',3), ('pt_PT',2)])\n",
    "    faker=Faker(locales)\n",
    "    np.random.seed(random_state)\n",
    "        \n",
    "    receiver_id_properties=[]\n",
    "    for receiver_id in range(n_receiver):\n",
    "\n",
    "        acc_id=faker.iban()\n",
    "        name=faker.first_name()\n",
    "        last_name=faker.last_name()\n",
    "        receiver_id_properties.append([receiver_id, acc_id, name, last_name])\n",
    "                                       \n",
    "    receiver_profiles_table = pd.DataFrame(receiver_id_properties, columns=['RECEIVER_ID','iban','name','last_name'])\n",
    "    \n",
    "    return receiver_profiles_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_receivers = 5\n",
    "receiver_profiles_table = generate_receiver_profiles_table(n_receivers, random_state = 0)\n",
    "receiver_profiles_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transactions_table(customer_profile, receiver_profiles_table, start_date = \"2017-09-05\", nb_days=10):\n",
    "    customer_transactions = []\n",
    "    \n",
    "    random.seed(int(customer_profile.SENDER_ID))\n",
    "    np.random.seed(int(customer_profile.SENDER_ID))\n",
    "    \n",
    "    # For all days\n",
    "    for day in range(nb_days):\n",
    "        \n",
    "        # Random number of transactions for that day \n",
    "        nb_tx = np.random.poisson(customer_profile.mean_nb_tx_per_day)\n",
    "\n",
    "        # If nb_tx positive, let us generate transactions\n",
    "        if nb_tx>0:\n",
    "            for tx in range(nb_tx):\n",
    "                \n",
    "                # Time of transaction: Around noon, std 20000 seconds. This choice aims at simulating the fact that \n",
    "                # most transactions occur during the day.\n",
    "                time_tx = int(np.random.normal(86400/2, 20000))\n",
    "                receiver_id = random.choice(receiver_profiles_table.RECEIVER_ID)\n",
    "                iban= receiver_profiles_table.loc[receiver_id,'iban'] \n",
    "                # If transaction time between 0 and 86400, let us keep it, otherwise, let us discard it\n",
    "                if (time_tx>0) and (time_tx<86400):\n",
    "                    \n",
    "                    # Amount is drawn from a normal distribution  \n",
    "                    amount = np.random.normal(customer_profile.mean_amount, customer_profile.std_amount)\n",
    "                    \n",
    "                    # If amount negative, draw from a uniform distribution\n",
    "                    if amount<0:\n",
    "                        amount = np.random.uniform(0,customer_profile.mean_amount*2)\n",
    "                    \n",
    "                    amount=np.round(amount,decimals=2)\n",
    "                        \n",
    "                    customer_transactions.append([time_tx+day*86400, day,customer_profile.SENDER_ID,\n",
    "                                                      customer_profile.iban, receiver_id,iban,amount]) \n",
    "                                                                  \n",
    "            \n",
    "    customer_transactions = pd.DataFrame(customer_transactions, columns=['TX_TIME_SECONDS', 'TX_TIME_DAYS', 'SENDER_ID','SENDER_IBAN','RECEIVER_ID','RECEIVER_IBAN','TX_AMOUNT'])\n",
    "    \n",
    "    if len(customer_transactions)>0:\n",
    "        customer_transactions['TX_DATETIME'] = pd.to_datetime(customer_transactions[\"TX_TIME_SECONDS\"], unit='s', origin=start_date)\n",
    "        customer_transactions=customer_transactions[['TX_DATETIME','SENDER_ID','SENDER_IBAN','RECEIVER_ID','RECEIVER_IBAN','TX_AMOUNT','TX_TIME_SECONDS', 'TX_TIME_DAYS']]\n",
    "    \n",
    "    return customer_transactions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Transactions generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_table_customer_0=generate_transactions_table(customer_profiles_table.iloc[0], receiver_profiles_table,\n",
    "                                                         start_date = \"2015-09-05\", \n",
    "                                                         nb_days = 5)\n",
    "transaction_table_customer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df=customer_profiles_table.groupby('SENDER_ID').apply(lambda x : generate_transactions_table(x.iloc[0], receiver_profiles_table, nb_days=5)).reset_index(drop=True)\n",
    "transactions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n_customers, n_receivers, nb_days, start_date=\"2017-09-05\"):\n",
    "    \n",
    "    start_time=time.time()\n",
    "    customer_profiles_table = generate_customer_profiles_table(n_customers, random_state = 0)\n",
    "    print(\"Time to generate customer profiles table: {0:.2}s\".format(time.time()-start_time))\n",
    "    \n",
    "    start_time=time.time()\n",
    "    receiver_profiles_table = generate_receiver_profiles_table(n_receivers, random_state = 1)\n",
    "    print(\"Time to generate receiver profiles table: {0:.2}s\".format(time.time()-start_time))\n",
    "    \n",
    "    start_time=time.time()\n",
    "    transactions_df=customer_profiles_table.groupby('SENDER_ID').apply(lambda x : generate_transactions_table(x.iloc[0], receiver_profiles_table, nb_days=nb_days)).reset_index(drop=True)\n",
    "    print(\"Time to generate transactions: {0:.2}s\".format(time.time()-start_time))\n",
    "    \n",
    "    # Sort transactions chronologically\n",
    "    transactions_df=transactions_df.sort_values('TX_DATETIME')\n",
    "    # Reset indices, starting from 0\n",
    "    transactions_df.reset_index(inplace=True,drop=True)\n",
    "    transactions_df.reset_index(inplace=True)\n",
    "    # TRANSACTION_ID are the dataframe indices, starting from 0\n",
    "    transactions_df.rename(columns = {'index':'TRANSACTION_ID'}, inplace = True)\n",
    "    \n",
    "    return (customer_profiles_table, receiver_profiles_table, transactions_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(customer_profiles_table, receiver_profiles_table, transactions_df)=\\\n",
    "    generate_dataset(n_customers = 6000, \n",
    "                     n_receivers = 7000, \n",
    "                     nb_days=1300, \n",
    "                     start_date=\"2017-09-05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(customer_profiles_table.style.to_latex(label='SENDERS TABLE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(receiver_profiles_table.style.to_latex(label='RECEIVERS TABLE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(transactions_df.style.to_latex(buf='transactions_df.tex'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fraud Scenario Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frauds(customer_profiles_table, receiver_profiles_table, transactions_df):\n",
    "    \n",
    "    # By default, all transactions are genuine\n",
    "    transactions_df['TX_FRAUD']=0\n",
    "    transactions_df['TX_FRAUD_SCENARIO']=0\n",
    "       \n",
    "    # Scenario 1, black-lists, mules. RECEIVERS. LEGITIMATE CLIENT, FRAUDULENT RECEIVER OR IBAN\n",
    "    for day in range(transactions_df.TX_TIME_DAYS.max()):\n",
    "        \n",
    "        compromised_receivers = receiver_profiles_table.RECEIVER_ID.sample(n=2, random_state=day).values\n",
    "        \n",
    "        compromised_transactions=transactions_df[transactions_df.RECEIVER_ID.isin(compromised_receivers)]\n",
    "                    \n",
    "        transactions_df.loc[compromised_transactions.index,'TX_FRAUD']=1\n",
    "        transactions_df.loc[compromised_transactions.index,'TX_FRAUD_SCENARIO']=1\n",
    "                                  \n",
    "    nb_frauds_scenario_1=transactions_df.TX_FRAUD.sum()\n",
    "    print(\"Number of frauds from scenario 1: \"+str(nb_frauds_scenario_1))\n",
    "    \n",
    "    #Scenario 2: phising cases, client account compromised\n",
    "    for day in range(transactions_df.TX_TIME_DAYS.max()):\n",
    "        \n",
    "        compromised_clients = customer_profiles_table.SENDER_ID.sample(n=5, random_state=day).values #5 clients compromised\n",
    "        \n",
    "        compromised_transactions=transactions_df[(transactions_df.TX_TIME_DAYS>=day) & \n",
    "                                                    (transactions_df.TX_TIME_DAYS<day+5) & \n",
    "                                                    (transactions_df.SENDER_ID.isin(compromised_clients))] #their transactions are compromised 5 days\n",
    "        \n",
    "        nb_compromised_transactions=len(compromised_transactions)\n",
    "        random.seed(day)\n",
    "        index_ftx= random.sample(list(compromised_transactions.index.values), k=int(nb_compromised_transactions/4)) #25% of their transactions are fraudulent because they have their amounts *1.15\n",
    "        \n",
    "        transactions_df.loc[index_ftx, 'TX_AMOUNT']= transactions_df.loc[index_ftx, 'TX_AMOUNT']*1.15\n",
    "        transactions_df.loc[index_ftx, 'TX_FRAUD']=1\n",
    "        transactions_df.loc[index_ftx, 'TX_FRAUD_SCENARIO']=2\n",
    "                     \n",
    "    nb_frauds_scenario_2=transactions_df.TX_FRAUD.sum()-nb_frauds_scenario_1\n",
    "    print(\"Number of frauds from scenario 2: \"+str(nb_frauds_scenario_2))\n",
    "     \n",
    "    return transactions_df                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time transactions_df = add_frauds(customer_profiles_table, receiver_profiles_table, transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.TX_FRAUD.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.TX_FRAUD.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df[transactions_df.TX_FRAUD_SCENARIO==2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df[transactions_df.TX_FRAUD_SCENARIO==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(transactions_df):\n",
    "    #Number of transactions per day\n",
    "    nb_tx_per_day=transactions_df.groupby(['TX_TIME_DAYS'])['SENDER_ID'].count()\n",
    "    print(nb_tx_per_day)\n",
    "    #print(nb_tx_per_day)\n",
    "    #Number of fraudulent transactions per day\n",
    "    nb_fraud_per_day=transactions_df.groupby(['TX_TIME_DAYS'])['TX_FRAUD'].sum()\n",
    "    #Number of fraudulent cards per day\n",
    "    nb_fraudbt_per_day=transactions_df[transactions_df['TX_FRAUD']>0].groupby(['TX_TIME_DAYS']).SENDER_ID.nunique()\n",
    "    \n",
    "    return (nb_tx_per_day,nb_fraud_per_day,nb_fraudbt_per_day)\n",
    "\n",
    "(nb_tx_per_day,nb_fraud_per_day,nb_fraudbt_per_day)=get_stats(transactions_df)\n",
    "\n",
    "n_days=len(nb_tx_per_day)\n",
    "print(n_days)\n",
    "tx_stats=pd.DataFrame({\"value\":pd.concat([nb_tx_per_day/50,nb_fraud_per_day/50,nb_fraudbt_per_day/50])})\n",
    "tx_stats['stat_type']=[\"nb_tx_per_day\"]*n_days+[\"nb_fraud_per_day\"]*n_days+[\"nb_fraudbt_per_day\"]*n_days\n",
    "tx_stats=tx_stats.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "sns.set(font_scale=1.4)\n",
    "\n",
    "fraud_and_transactions_stats_fig = plt.gcf()\n",
    "\n",
    "fraud_and_transactions_stats_fig.set_size_inches(15, 8)\n",
    "\n",
    "sns_plot = sns.lineplot(x=\"TX_TIME_DAYS\", y=\"value\", data=tx_stats, hue=\"stat_type\", hue_order=[\"nb_tx_per_day\",\"nb_fraud_per_day\",\"nb_fraudbt_per_day\"], legend=False)\n",
    "\n",
    "sns_plot.set_title('Total transactions, and number of fraudulent transactions \\n and number of compromised transfers per day', fontsize=20)\n",
    "sns_plot.set(xlabel = \"Number of days since beginning of data generation\", ylabel=\"Number\")\n",
    "\n",
    "sns_plot.set_ylim([0,500])\n",
    "\n",
    "labels_legend = [\"# transactions per day (/50)\", \"# fraudulent txs per day (/50)\", \"# fraudulent transfers per day (/50)\"]\n",
    "\n",
    "sns_plot.legend(loc='upper left', labels=labels_legend,bbox_to_anchor=(1.05, 1), fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_and_transactions_stats_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving dataset without Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_OUTPUT = \"./simulated-data-raw/\"\n",
    "\n",
    "if not os.path.exists(DIR_OUTPUT):\n",
    "    os.makedirs(DIR_OUTPUT)\n",
    "\n",
    "start_date = datetime.datetime.strptime(\"2017-09-05\", \"%Y-%m-%d\")\n",
    "\n",
    "for day in range(transactions_df.TX_TIME_DAYS.max()+1):\n",
    "    \n",
    "    transactions_day = transactions_df[transactions_df.TX_TIME_DAYS==day].sort_values('TX_TIME_SECONDS')\n",
    "    \n",
    "    date = start_date + datetime.timedelta(days=day)\n",
    "    filename_output = date.strftime(\"%Y-%m-%d\")+'.pkl'\n",
    "    \n",
    "    # Protocol=4. It adds support for very large objects, pickling more kinds of objects, and some data format optimizations. It is the default protocol starting with Python 3.8. \n",
    "    transactions_day.to_pickle(DIR_OUTPUT+filename_output, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(transactions_df.style.to_latex(buf='transactions_df_frauds.tex'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to read pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a set of pickle files, put them together in a single DataFrame, and order them by time\n",
    "# It takes as input the folder DIR_INPUT where the files are stored, and the BEGIN_DATE and END_DATE\n",
    "#import pandas as pd\n",
    "\n",
    "def read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE):\n",
    "    \n",
    "    files = [os.path.join(DIR_INPUT, f) for f in os.listdir(DIR_INPUT) if f>=BEGIN_DATE+'.pkl' and f<=END_DATE+'.pkl']\n",
    "\n",
    "    frames = []\n",
    "    for f in files:\n",
    "        df = pd.read_pickle(f)\n",
    "        frames.append(df)\n",
    "        del df\n",
    "    df_final = pd.concat(frames)\n",
    "    \n",
    "    df_final=df_final.sort_values('TRANSACTION_ID')\n",
    "    df_final.reset_index(drop=True,inplace=True)\n",
    "    #  Note: -1 are missing values for real world data \n",
    "    df_final=df_final.replace([-1],0)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_INPUT='./simulated-data-raw' \n",
    "\n",
    "BEGIN_DATE = \"2017-09-05\"\n",
    "END_DATE = \"2021-03-27\"\n",
    "\n",
    "print(\"Load  files\")\n",
    "%time transactions_df=read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)\n",
    "print(\"{0} transactions loaded, containing {1} fraudulent transactions\".format(len(transactions_df),transactions_df.TX_FRAUD.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TX_DURING_WEEKEND feature\n",
    "## 1 -> wend, 0 -> otherwise\n",
    "def is_weekend(tx_datetime):\n",
    "    \n",
    "    # Transform date into weekday (0 is Monday, 6 is Sunday)\n",
    "    weekday = tx_datetime.weekday()\n",
    "    # Binary value: 0 if weekday, 1 if weekend\n",
    "    is_weekend = weekday>=5\n",
    "    \n",
    "    return int(is_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df['TX_DURING_WEEKEND']=transactions_df.TX_DATETIME.apply(is_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 -> night, 0 -> otherwise(day)\n",
    "def is_night(tx_datetime):\n",
    "    \n",
    "    # Get the hour of the transaction\n",
    "    tx_hour = tx_datetime.hour\n",
    "    # Binary value: 1 if hour less than 6, and 0 otherwise\n",
    "    is_night = tx_hour<=6\n",
    "    \n",
    "    return int(is_night)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time transactions_df['TX_DURING_NIGHT']=transactions_df.TX_DATETIME.apply(is_night)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df[transactions_df.TX_TIME_DAYS>=30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transactions_df.style.to_latex(buf='tx_datefeature.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##sender id transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customer_spending_behaviour_features(customer_transactions, windows_size_in_days=[1,7,30]):\n",
    "    \n",
    "    # Let us first order transactions chronologically\n",
    "    customer_transactions=customer_transactions.sort_values('TX_DATETIME')\n",
    "    \n",
    "    # The transaction date and time is set as the index, which will allow the use of the rolling function \n",
    "    customer_transactions.index=customer_transactions.TX_DATETIME\n",
    "    \n",
    "    # For each window size\n",
    "    for window_size in windows_size_in_days:\n",
    "        \n",
    "        # Compute the sum of the transaction amounts and the number of transactions for the given window size\n",
    "        SUM_AMOUNT_TX_WINDOW=customer_transactions['TX_AMOUNT'].rolling(str(window_size)+'d').sum()\n",
    "        NB_TX_WINDOW=customer_transactions['TX_AMOUNT'].rolling(str(window_size)+'d').count()\n",
    "    \n",
    "        # Compute the average transaction amount for the given window size\n",
    "        # NB_TX_WINDOW is always >0 since current transaction is always included\n",
    "        AVG_AMOUNT_TX_WINDOW=SUM_AMOUNT_TX_WINDOW/NB_TX_WINDOW\n",
    "    \n",
    "        # Save feature values\n",
    "        customer_transactions['SENDER_ID_NB_TX_'+str(window_size)+'DAY_WINDOW']=list(NB_TX_WINDOW)\n",
    "        customer_transactions['SENDER_ID_AVG_AMOUNT_'+str(window_size)+'DAY_WINDOW']=list(AVG_AMOUNT_TX_WINDOW)\n",
    "    \n",
    "    # Reindex according to transaction IDs\n",
    "    customer_transactions.index=customer_transactions.TRANSACTION_ID\n",
    "        \n",
    "    # And return the dataframe with the new features\n",
    "    return customer_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spending_behaviour_customer_0=get_customer_spending_behaviour_features(transactions_df[transactions_df.SENDER_ID==0])\n",
    "spending_behaviour_customer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time transactions_df=transactions_df.groupby('SENDER_ID').apply(lambda x: get_customer_spending_behaviour_features(x, windows_size_in_days=[1,7,30]))\n",
    "transactions_df=transactions_df.sort_values('TX_DATETIME').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transactions_df.style.to_latex(buf='tx_senderfeature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECEIVER TRANSFORMATIONS\n",
    "#### DELAY PERIOD: TIME TO DISCOVER THE FRAUDULENT TRANSACTIONS, CUSTOMER COMPLAINT, OR INVESTIGATION. SET TO 1 WEEK FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_risk_rolling_window(receiver_transactions, delay_period=7, windows_size_in_days=[1,7,30], feature=\"RECEIVER_ID\"):\n",
    "    \n",
    "    receiver_transactions=receiver_transactions.sort_values('TX_DATETIME')\n",
    "    \n",
    "    receiver_transactions.index=receiver_transactions.TX_DATETIME\n",
    "    \n",
    "    NB_FRAUD_DELAY=receiver_transactions['TX_FRAUD'].rolling(str(delay_period)+'d').sum()\n",
    "    NB_TX_DELAY=receiver_transactions['TX_FRAUD'].rolling(str(delay_period)+'d').count()\n",
    "    \n",
    "    for window_size in windows_size_in_days:\n",
    "    \n",
    "        NB_FRAUD_DELAY_WINDOW=receiver_transactions['TX_FRAUD'].rolling(str(delay_period+window_size)+'d').sum()\n",
    "        NB_TX_DELAY_WINDOW=receiver_transactions['TX_FRAUD'].rolling(str(delay_period+window_size)+'d').count()\n",
    "    \n",
    "        NB_FRAUD_WINDOW=NB_FRAUD_DELAY_WINDOW-NB_FRAUD_DELAY\n",
    "        NB_TX_WINDOW=NB_TX_DELAY_WINDOW-NB_TX_DELAY\n",
    "    \n",
    "        RISK_WINDOW=NB_FRAUD_WINDOW/NB_TX_WINDOW # Risk score\n",
    "        \n",
    "        receiver_transactions[feature+'_NB_TX_'+str(window_size)+'DAY_WINDOW']=list(NB_TX_WINDOW)\n",
    "        receiver_transactions[feature+'_RISK_'+str(window_size)+'DAY_WINDOW']=list(RISK_WINDOW)\n",
    "        \n",
    "    receiver_transactions.index=receiver_transactions.TRANSACTION_ID\n",
    "    \n",
    "    # Replace NA values with 0 (all undefined risk scores where NB_TX_WINDOW is 0) \n",
    "    receiver_transactions.fillna(0,inplace=True)\n",
    "    \n",
    "    return receiver_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df[transactions_df.TX_FRAUD==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_count_risk_rolling_window(transactions_df[transactions_df.RECEIVER_ID==3059], delay_period=7, windows_size_in_days=[1,7,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time transactions_df=transactions_ df.groupby('RECEIVER_ID').apply(lambda x: get_count_risk_rolling_window(x, delay_period=7, windows_size_in_days=[1,7,30], feature=\"RECEIVER_ID\"))\n",
    "transactions_df=transactions_df.sort_values('TX_DATETIME').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(transactions_df.style.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving dataset with Feature Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_OUTPUT = \"./simulated-data-transformed/\"\n",
    "\n",
    "if not os.path.exists(DIR_OUTPUT):\n",
    "    os.makedirs(DIR_OUTPUT)\n",
    "\n",
    "start_date = datetime.datetime.strptime(\"2017-09-05\", \"%Y-%m-%d\")\n",
    "\n",
    "for day in range(transactions_df.TX_TIME_DAYS.max()+1):\n",
    "    \n",
    "    transactions_day = transactions_df[transactions_df.TX_TIME_DAYS==day].sort_values('TX_TIME_SECONDS')\n",
    "    \n",
    "    date = start_date + datetime.timedelta(days=day)\n",
    "    filename_output = date.strftime(\"%Y-%m-%d\")+'.pkl'\n",
    "    \n",
    "    # Protocol=4 required for Google Colab\n",
    "    transactions_day.to_pickle(DIR_OUTPUT+filename_output, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the 2021-03-23 to the 2021-03-27\n",
    "\n",
    "DIR_INPUT='./simulated-data-transformed' \n",
    "\n",
    "BEGIN_DATE = \"2021-03-23\"\n",
    "END_DATE = \"2021-03-27\"\n",
    "\n",
    "print(\"Load  files\")\n",
    "%time transactions_df=read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)\n",
    "print(\"{0} transactions loaded, containing {1} fraudulent transactions\".format(len(transactions_df),transactions_df.TX_FRAUD.sum()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e06ff7da33dc9620448857a90ad8b5f428f0d573d205a934d2841c8aee45ea32"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
